@article{Rose2013,
author = {Rose, Kenneth H.},
doi = {10.1002/pmj.21345},
isbn = {9781628251845},
issn = {87569728},
journal = {Project Management Journal},
month = {jun},
number = {3},
pages = {e1--e1},
publisher = {Newtown Square},
title = {{A Guide to the Project Management Body of Knowledge (PMBOK{\textregistered} Guide)-Fifth Edition}},
url = {http://doi.wiley.com/10.1002/pmj.21345},
volume = {44},
year = {2013}
}
@techreport{Wickham,
author = {Wickham, Hadley},
file = {:G$\backslash$:/Shared drives/Library/Mendeley/Wickham, Unknown, Analysis projects.pdf:pdf},
pages = {1--10},
title = {{Analysis projects}},
url = {https://docs.google.com/document/d/1aZJf2rGf8SGAPLvWZa4PkWNfRpatMU8zpN0W3WjgHp4/edit{\#}}
}
@book{Ousterhout2018,
abstract = {This book addresses the topic of software design: how to decompose complex software systems into modules (such as classes and methods) that can be implemented relatively independently. The book first introduces the fundamental problem in software design, which is managing complexity. It then discusses philosophical issues about how to approach the software design process, and it presents a collection of design principles to apply during software design. The book also introduces a set of red flags that identify design problems. You can apply the ideas in this book to minimize the complexity of large software systems, so that you can write software more quickly and cheaply.},
author = {Ousterhout, John},
isbn = {978-1732102200},
pages = {190},
publisher = {Yaknyam Press},
title = {{A Philosophy of Software Design}},
url = {https://b-ok.cc/book/3701141/98e349},
year = {2018}
}
@book{Vernon2013,
author = {Vernon, Vaughn},
edition = {1st Editio},
isbn = {9780321834577},
keywords = {DDD,IDDD},
mendeley-tags = {DDD,IDDD},
pages = {656},
publisher = {Addison-Wesley Professional},
title = {{Implementing Domain-Driven Design}},
year = {2013}
}
@misc{Drivendata2015,
author = {Drivendata},
file = {:G$\backslash$:/Shared drives/Library/Mendeley/Drivendata, 2015, Cookiecutter Data Science.pdf:pdf},
title = {{Cookiecutter Data Science}},
url = {https://drivendata.github.io/cookiecutter-data-science/},
year = {2015}
}
@book{Evans2004,
author = {Evans, Eric},
keywords = {DDD,architecture,evans2004domain},
mendeley-tags = {evans2004domain},
publisher = {Addison-Wesley Professional},
title = {{Domain-driven design: tackling complexity in the heart of software}},
year = {2004}
}
@book{Fay2020,
author = {Fay, Colin and Rochette, S{\'{e}}bastien and Guyader, Vincent and Girard, Cervan},
keywords = {golem},
mendeley-tags = {golem},
publisher = {Chapman {\&} Hall},
title = {{Engineering Production-Grade Shiny Apps}},
url = {https://engineering-shiny.org/},
year = {2020}
}
@misc{Koen2019,
author = {Koen, Semi},
file = {:G$\backslash$:/Shared drives/Library/Mendeley/Koen, 2019, How to build scalable Machine Learning systems.pdf:pdf},
title = {{How to build scalable Machine Learning systems}},
url = {https://towardsdatascience.com/being-a-data-scientist-does-not-make-you-a-software-engineer-c64081526372 https://towardsdatascience.com/architecting-a-machine-learning-pipeline-a847f094d1c7 https://i.imgur.com/y7xw233.jpg},
year = {2019}
}
@book{Phillips2018,
author = {Phillips, Nathaniel D.},
publisher = {Observer},
title = {{Yarrr! The pirate's guide to R.}},
url = {https://bookdown.org/ndphillips/YaRrr/},
year = {2018}
}
@article{Baylor2017,
abstract = {Creating and maintaining a platform for reliably producing and deploying machine learning models requires careful or-chestration of many componentsâ€”a learner for generating models based on training data, modules for analyzing and val-idating both data as well as models, and finally infrastructure for serving models in production. This becomes particularly challenging when data changes over time and fresh models need to be produced continuously. Unfortunately, such or-chestration is often done ad hoc using glue code and custom scripts developed by individual teams for specific use cases, leading to duplicated effort and fragile systems with high technical debt. We present TensorFlow Extended (TFX), a TensorFlow-based general-purpose machine learning platform implemented at Google. By integrating the aforementioned components into one platform, we were able to standardize the compo-nents, simplify the platform configuration, and reduce the time to production from the order of months to weeks, while providing platform stability that minimizes disruptions. We present the case study of one deployment of TFX in the Google Play app store, where the machine learning models are refreshed continuously as new data arrive. Deploying TFX led to reduced custom code, faster experiment cycles, and a 2{\%} increase in app installs resulting from improved data and model analysis.},
author = {Baylor, Denis and Breck, Eric and Cheng, Heng-Tze and Fiedel, Noah and {Yu Foo}, Chuan and Haque, Zakaria and Haykal, Salem and Ispir, Mustafa and Jain, Vihan and Koc, Levent and {Yuen Koo}, Chiu and Lew, Lukasz and Mewald, Clemens and {Naresh Modi}, Akshay and Polyzotis, Neoklis and Ramesh, Sukriti and Roy, Sudip and {Euijong Whang}, Steven and Wicke, Martin and Wilkiewicz, Jarek and Zhang, Xin and {Zinkevich Google Inc}, Martin},
doi = {10.1145/3097983.3098021},
file = {:G$\backslash$:/Shared drives/Library/Mendeley/Baylor et al., 2017, TFX A TensorFlow-Based Production-Scale Machine Learning Platform.pdf:pdf},
isbn = {9781450348874},
journal = {Kdd},
keywords = {TFX,TensorFlow,continuous training,end-to-end platform,large-scale machine learning},
mendeley-tags = {TFX,TensorFlow},
pages = {1387--1395},
title = {{TFX: A TensorFlow-Based Production-Scale Machine Learning Platform}},
url = {http://delivery.acm.org/10.1145/3100000/3098021/p1387-baylor.pdf?ip=149.132.25.65{\&}id=3098021{\&}acc=OPENTOC{\&}key=296E2ED678667973.B9B72D0607302676.4D4702B0C3E38B35.054E54E275136550{\&}CFID=986587823{\&}CFTOKEN=28999827{\&}{\_}{\_}acm{\_}{\_}=1507019813{\_}ab7be4a96be057630413a29a111},
year = {2017}
}
@article{FarhadForoughi2018,
abstract = {Cyber-security solutions are traditionally static and signature-based. The traditional solutions along with the use of analytic models, machine learning and big data could be improved by automatically trigger mitigation or provide relevant awareness to control or limit consequences of threats. This kind of intelligent solutions is covered in the context of Data Science for Cyber-security. Data Science provides a significant role in cyber-security by utilising the power of data (and big data), high-performance computing and data mining (and machine learning) to protect users against cyber-crimes. For this purpose, a successful data science project requires an effective methodology to cover all issues and provide adequate resources. In this paper, we are introducing popular data science methodologies and will compare them in accordance with cyber-security challenges. A comparison discussion has also delivered to explain methodologies strengths and weaknesses in case of cyber-security projects.},
author = {{Farhad Foroughi}, Peter Luksch},
doi = {10.5121/ijdms.2011.3105},
file = {:G$\backslash$:/Shared drives/Library/Mendeley/Farhad Foroughi, 2018, Data Science Methodology For Cybersecurity Projects.pdf:pdf},
journal = {arXiv},
title = {{Data Science Methodology For Cybersecurity Projects}},
url = {https://arxiv.org/abs/1803.04219},
year = {2018}
}
@article{Baer2019,
abstract = {Predictive modeling has an increasing number of applications in various fields. High demand for predictive models drives creation of tools that automate and support work of data scientist on the model development. To better understand what can be automated we need first a description of the model life-cycle. In this paper we propose a generic Model Development Process (MDP). This process is inspired by Rational Unified Process (RUP) which was designed for software development. There are other approached to process description, like CRISP DM or ASUM DM, in this paper we discuss similarities and differences between these methodologies. We believe that the proposed open standard for model development will facilitate creation of tools for automation of model training, testing and maintaining.},
archivePrefix = {arXiv},
arxivId = {arXiv:1907.04461v1},
author = {Baer, Tobias and Baer, Tobias},
doi = {10.1007/978-1-4842-4885-0_4},
eprint = {arXiv:1907.04461v1},
file = {:G$\backslash$:/Shared drives/Library/Mendeley/Baer, Baer, 2019, The Model Development Process.pdf:pdf},
journal = {Understand, Manage, and Prevent Algorithmic Bias},
pages = {29--39},
title = {{The Model Development Process}},
year = {2019}
}
@article{Moore2009,
abstract = {The article discusses the history of Appraisal Theory and Practice since the International Association of Assessing Officers (IAAO) was organized. It traces the evolution of appraisal practice from the start of concepts of the approaches to computerized valuation and until the evolution of automated valuation models (AVMs). It states how the economic and valuation theories affected the development of the appraisal methods and how the social forces have shaped the assessment policy and practices.},
author = {Moore, Jw},
file = {:G$\backslash$:/Shared drives/Library/Mendeley/Moore, 2009, A History of Appraisal Theory and Practice Looking Back from IAAO's 75th Year.pdf:pdf},
isbn = {13571419},
issn = {07421303},
journal = {Journal of Property Tax Assessment {\&} Administration},
number = {3},
pages = {23--50},
pmid = {44147902},
title = {{A History of Appraisal Theory and Practice Looking Back from IAAO's 75th Year}},
volume = {6},
year = {2009}
}
@misc{Microsoft2017,
abstract = {The Team Data Science Process is an agile, iterative data science methodology to deliver predictive analytics solutions and intelligent applications efficiently. TDSP helps improve team collaboration and learning. It contains a distillation of the best practices and structures from Microsoft and others in the industry that facilitate the successful implementation of data science initiatives. The goal is to help companies fully realize the benefits of their analytics program.},
author = {Microsoft},
booktitle = {WD info},
file = {:G$\backslash$:/Shared drives/Library/Mendeley/Microsoft, 2017, Team Data Science Process Documentation.pdf:pdf},
pages = {600},
title = {{Team Data Science Process Documentation}},
url = {https://docs.microsoft.com/en-us/azure/machine-learning/team-data-science-process/},
year = {2017}
}
@book{BrooksJr1995,
author = {{Brooks Jr}, Frederick P},
edition = {2},
publisher = {Pearson Education},
title = {{The mythical man-month: essays on software engineering}},
year = {1995}
}
@article{Marwick2018,
abstract = {Computers are a central tool in the research process, enabling complex and large-scale data analysis. As computer-based research has increased in complexity, so have the challenges of ensuring that this research is reproducible. To address this challenge, we review the concept of the research compendium as a solution for providing a standard and easily recognizable way for organizing the digital materials of a research project to enable other researchers to inspect, reproduce, and extend the research. We investigate how the structure and tooling of software packages of the R programming language are being used to produce research compendia in a variety of disciplines. We also describe how software engineering tools and services are being used by researchers to streamline working with research compendia. Using real-world examples, we show how researchers can improve the reproducibility of their work using research compendia based on R packages and related tools.},
author = {Marwick, Ben and Boettiger, Carl and Mullen, Lincoln},
doi = {10.1080/00031305.2017.1375986},
file = {:G$\backslash$:/Shared drives/Library/Mendeley/Marwick, Boettiger, Mullen, 2018, Packaging Data Analytical Work Reproducibly Using R (and Friends).pdf:pdf},
issn = {15372731},
journal = {American Statistician},
keywords = {Computational science,Data science,Open source software,Reproducible research},
number = {1},
pages = {80--88},
title = {{Packaging Data Analytical Work Reproducibly Using R (and Friends)}},
volume = {72},
year = {2018}
}
@book{Wickham2015,
author = {Wickham, Hadley and Bryan, Jennifer},
keywords = {R packages},
mendeley-tags = {R packages},
publisher = {O'Reilly Media, Inc},
title = {{R packages: organize, test, document, and share your code}},
url = {https://r-pkgs.org/},
year = {2015}
}
@incollection{SculleyDandHoltGaryandGolovinDanielandDavydovEugeneandPhillipsToddandEbnerDietmarandChaudharyVinayandYoungMichaelandCrespoJean-FrancoisandDennison2015,
author = {{Sculley, D and Holt, Gary and Golovin, Daniel and Davydov, Eugene and Phillips, Todd and Ebner, Dietmar and Chaudhary, Vinay and Young, Michael and Crespo, Jean-Francois and Dennison}, Dan},
booktitle = {Advances in neural information processing systems},
file = {:G$\backslash$:/Shared drives/Library/Mendeley/Sculley, D and Holt, Gary and Golovin, Daniel and Davydov, Eugene and Phillips, Todd and Ebner, Dietmar and Chaudhary, Vinay and Young,.pdf:pdf},
isbn = {0262017091, 9780262017091},
issn = {10495258},
pages = {2503----2511},
title = {{Hidden technical debt in machine learning systems}},
url = {http://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf},
year = {2015}
}
@incollection{Angee2018,
address = {Cham},
author = {Ang{\'{e}}e, Santiago and Lozano-Argel, Silvia I. and Montoya-Munera, Edwin N. and Ospina-Arango, Juan-David and Tabares-Betancur, Marta S.},
booktitle = {2018 First International Conference on Artificial Intelligence for Industries (AI4I)},
doi = {10.1007/978-3-319-95204-8_51},
editor = {Uden, Lorna and Hadzima, Branislav and Ting, I-Hsien},
file = {:G$\backslash$:/Shared drives/Library/Mendeley/Ang{\'{e}}e et al., 2018, Towards an Improved ASUM-DM Process Methodology for Cross-Disciplinary Multi-organization Big Data {\&}amp Analytics Pr.pdf:pdf},
isbn = {978-3-319-95203-1},
keywords = {case-based reasoning system,semantic analysis},
number = {July},
pages = {613--624},
publisher = {Springer International Publishing},
series = {Communications in Computer and Information Science},
title = {{Towards an Improved ASUM-DM Process Methodology for Cross-Disciplinary Multi-organization Big Data {\&} Analytics Projects}},
volume = {877},
year = {2018}
}
@misc{Vickery2020,
abstract = {towardsdatascience},
author = {Vickery, Rebecca},
file = {:G$\backslash$:/Shared drives/Library/Mendeley/Vickery, 2020, A Recipe for Organising Data Science Projects.pdf:pdf},
title = {{A Recipe for Organising Data Science Projects}},
url = {https://towardsdatascience.com/a-recipe-for-organising-data-science-projects-50a1cc539c69},
year = {2020}
}
@book{Vernon2016,
author = {Vernon, Vaughn},
edition = {1st Editio},
isbn = {0134434420},
keywords = {DDD,DDDD},
mendeley-tags = {DDD,DDDD},
pages = {166},
publisher = {Addison-Wesley Professional},
title = {{Domain-Driven Design Distilled}},
year = {2016}
}
@book{Percival2020,
author = {Percival, Harry and Gregory, Bob},
editor = {Edition, 1},
isbn = {978-1492052203},
keywords = {DDD,PDDD},
mendeley-tags = {DDD,PDDD},
pages = {304},
publisher = {O'Reilly Media, Inc.},
title = {{Architecture Patterns with Python Enabling Test-Driven Development, Domain-Driven Design, and Event-Driven Microservice}},
year = {2020}
}
@misc{RCoreTeam2018,
abstract = {This is a guide to extending R, describing the process of creating R add-on packages, writing R documentation, R's system and foreign language interfaces, and the R API.},
author = {{R Core Team}},
title = {{Writing R Extensions}},
url = {https://rstudio.github.io/r-manuals/r-exts/},
year = {2018}
}
@book{Martin2017,
author = {Martin, Robert C},
isbn = {9780134494166},
keywords = {architecture,clean},
pages = {400},
publisher = {Prentice Hall Press},
title = {{Clean architecture: a craftsman's guide to software structure and design}},
url = {https://books.google.co.nz/books/about/Clean{\_}Architecture.html?id=8ngAkAEACAAJ{\&}source=kp{\_}book{\_}description{\&}redir{\_}esc=y},
year = {2017}
}
@book{Wickham2017,
author = {Wickham, Hadley and Grolemund, Garrett},
isbn = {978-1-4919-1039-9},
keywords = {R for Data Science},
mendeley-tags = {R for Data Science},
pages = {520},
publisher = {O'Reilly Media, Inc.},
title = {{R for Data Science: Import, Tidy, Transform, Visualize, and Model Data}},
url = {https://r4ds.had.co.nz/},
year = {2017}
}
@article{Shearer2000,
abstract = {This article describes CRISP-DM (CRoss-Industry Standard Process for Data Mining), a non-proprietary, documented, and freely available data mining model. Developed by indus- try leaders with input from more than 200 data mining users and data mining tool and service providers, CRISP-DM is an industry-, tool-, and application-neutral model. This model encourages best practices and offers organizations the struc- ture needed to realize better, faster results from data mining. CRISP-DM organizes the data mining process into six phases: business understanding, data understanding, data prepara- tion, modeling, evaluation, and deployment. These phases help organizations understand the data mining process and provide a road map to follow while planning and carrying out a data mining project. This article explores all six phases, including the tasks involved with each phase. Sidebar materi- al, which takes a look at specific data mining problem types and techniques for addressing them, is provided.},
author = {Shearer, Colin and Watson, Hugh J and Grecich, Daryl G and Moss, Larissa and Adelman, Sid and Hammer, Katherine and Herdlein, Stacey a},
file = {:G$\backslash$:/Shared drives/Library/Mendeley/Shearer et al., 2000, The CRIS-DM model The New Blueprint for Data Mining.pdf:pdf},
isbn = {1092-6208},
journal = {Journal of Data Warehousing14},
keywords = {CRISP-DM Model,miner mineration},
number = {4},
pages = {13--22},
title = {{The CRIS-DM model: The New Blueprint for Data Mining}},
url = {www.spss.com{\%}5Cnwww.dw-institute.com},
volume = {5},
year = {2000}
}
@article{Ahmed2019,
abstract = {As data projects become more conventional, increase in the use of information has surpassed the knowledge of how to support individuals/teams that undertake such projects. Leading data mining methodology, CRISP-DM has become limited in managing the requirements of working with recent technologies such as Machine Learning. Resultantly, many have either created their own methods or adopted alternative approaches such as the Design Thinking and Lean Startup innovation strategies. Consequently, this paper proposes a novel software development methodology entitled Lean Design Thinking Methodology (LDTM) to guide the development of modern data projects. LDTM combines the strengths of CRISP-DM with the more innovative Design Thinking and Lean Startup strategies to introduce an approach divided into three stages, comprising of seven steps. This paper concludes on how there is no one correct method, nor is one single approach enough, but together, elements of each approach can unite to help guide data projects forward.},
author = {Ahmed, Bakhtiyar and Dannhauser, Thomas and Philip, Nada},
doi = {10.1109/CEEC.2018.8674234},
file = {:G$\backslash$:/Shared drives/Library/Mendeley/Ahmed, Dannhauser, Philip, 2019, A Lean Design Thinking Methodology (LDTM) for Machine Learning and Modern Data Projects.pdf:pdf},
isbn = {9781538672754},
journal = {2018 10th Computer Science and Electronic Engineering Conference, CEEC 2018 - Proceedings},
keywords = {Data Mining,Framework,Life Cycle,Machine Learning,Methodology,Software Development},
pages = {11--14},
title = {{A Lean Design Thinking Methodology (LDTM) for Machine Learning and Modern Data Projects}},
year = {2019}
}
